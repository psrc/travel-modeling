{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script computes emissions within city boundaries using standard Soundcast network outputs and established emissions rates for base and forecast years. Additional functions are provided from the imported \"emissions\" and \"functions\" methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os.path\n",
    "\n",
    "from functions import read_from_sde, load_network_summary, intersect_geog\n",
    "from emissions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root of model run to analyze AND the model year\n",
    "run_dir = r'L:\\RTP_2022\\final_runs\\sc_rtp_2018_final\\soundcast'\n",
    "model_year = '2018'    # Make sure to update this since rates used are based on this value\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_rtp_2030_final\\soundcast'\n",
    "# model_year = '2030'    # Make sure to update this since rates used are based on this value\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_2040_rtp_final\\soundcast'\n",
    "# model_year = '2040'\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "# model_year = '2050'\n",
    "\n",
    "# Set output directory; results will be stored in a folder by model year \n",
    "output_dir = r'C:\\Workspace\\aq_tool\\output\\\\' + model_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Change working directory to run_dir\n",
    "os.chdir(run_dir)\n",
    "\n",
    "# Load the network\n",
    "crs = 'EPSG:2285'\n",
    "gdf_network = gpd.read_file(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_edges.shp'))\n",
    "gdf_network.crs = crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  tract geographies from ElmerGeo\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "\n",
    "version = \"'DBO.Default'\"\n",
    "gdf_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parcels as a geodataframe\n",
    "parcel_df = pd.read_csv(os.path.join(run_dir,r'inputs/scenario/landuse/parcels_urbansim.txt'), delim_whitespace=True,\n",
    "                            usecols=['PARCELID','XCOORD_P','YCOORD_P'])\n",
    "\n",
    "parcel_gdf = gpd.GeoDataFrame(parcel_df,\n",
    "        geometry=gpd.points_from_xy(parcel_df['XCOORD_P'], parcel_df['YCOORD_P']), crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform intersect to get the network within each city in a list\n",
    "df_network = load_network_summary(os.path.join(run_dir, r'outputs\\network\\network_results.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_emissions(_gdf_shp, model_year):\n",
    "    \"\"\"Compute emissions for a jurisdiction. \n",
    "    \n",
    "        _gdf_shp: polygon geodataframe of a jurisidiction (should be complete coverage without holes over bodies of water, etc.)\n",
    "        model_year: must be passed for clarity\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Intersect jurisdiction polygon with network shapefile and network CSV file\n",
    "    _gdf_shp = intersect_geog(_gdf_shp, gdf_network, df_network)\n",
    "\n",
    "    # Select links from network summary dataframe that are within the gdf_shp\n",
    "    # The dataframe contains link-level model outputs\n",
    "    _df = df_network[df_network['ij'].isin(_gdf_shp['id'])]\n",
    "\n",
    "    # Replace length with length from _gdf_shp to ensure roads stop at city boundaries\n",
    "    # Drop \"length field from _gdf_shp, which is length in miles; \n",
    "    # use new_length field, which is calculated length of intersected links in feet\n",
    "    _df.drop('length', axis=1, inplace=True)\n",
    "    _df = _df.merge(_gdf_shp, how='left', left_on='ij', right_on='id')\n",
    "    # _df.drop('length', axis=1, inplace=True)\n",
    "    _df.rename(columns={'new_length': 'length',\n",
    "                    'length': 'original_length'}, inplace=True)\n",
    "\n",
    "    # Calculate interzonal emissions using same approach as for regional/county emissions\n",
    "    os.chdir(run_dir)\n",
    "    conn = create_engine('sqlite:///inputs/db/soundcast_inputs.db')\n",
    "\n",
    "    # Load running emission rates by vehicle type, for the model year\n",
    "    df_running_rates = pd.read_sql('SELECT * FROM running_emission_rates_by_veh_type WHERE year=='+model_year, con=conn)\n",
    "    df_running_rates.rename(columns={'ratePerDistance': 'grams_per_mile'}, inplace=True)\n",
    "    df_running_rates['year'] = df_running_rates['year'].astype('str')\n",
    "\n",
    "    # Select the month to use for each pollutant; some rates are used for winter or summer depending\n",
    "    # on when the impacts are at a maximum due to temperature.\n",
    "    df_summer = df_running_rates[df_running_rates['pollutantID'].isin(summer_list)]\n",
    "    df_summer = df_summer[df_summer['monthID'] == 7]\n",
    "    df_winter = df_running_rates[~df_running_rates['pollutantID'].isin(summer_list)]\n",
    "    df_winter = df_winter[df_winter['monthID'] == 1]\n",
    "    df_running_rates = df_winter.append(df_summer)\n",
    "\n",
    "    df_interzonal_vmt = calculate_interzonal_vmt(_df)\n",
    "    df_interzonal = calculate_interzonal_emissions(df_interzonal_vmt, df_running_rates)\n",
    "\n",
    "    # Get list of zones to include for intrazonal trips\n",
    "    # Include intrazonal trips for any TAZ centroids within city boundary\n",
    "\n",
    "    # Load TAZ centroids\n",
    "    connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "\n",
    "    version = \"'DBO.Default'\"\n",
    "    taz_gdf = read_from_sde(connection_string, 'taz2010_no_water', version, crs=crs, is_table=False)\n",
    "\n",
    "    _taz_gdf = gpd.GeoDataFrame(taz_gdf.centroid)\n",
    "    _taz_gdf.geometry = _taz_gdf[0]\n",
    "    _taz_gdf['taz'] = taz_gdf['taz'].astype('int')\n",
    "\n",
    "    city_gdf = gdf_shp[gdf_shp['juris'] == city]\n",
    "\n",
    "    intersect_gdf = gpd.overlay(_taz_gdf, \n",
    "            city_gdf, \n",
    "            how=\"intersection\")\n",
    "\n",
    "    # Load intrazonal trips for zones in the area\n",
    "    df_iz = pd.read_csv(os.path.join(run_dir,r'outputs\\network\\iz_vol.csv'))\n",
    "\n",
    "    # Filter for zone centroids within the jurisdiction\n",
    "    _df_iz = df_iz[df_iz['taz'].isin(intersect_gdf.taz)]\n",
    "\n",
    "    # If no zones centroids in a city, pass an empty df with 0 values for VMT;\n",
    "    # Otherwise calculate intrazonal VMT for the associated TAZs only\n",
    "    if len(_df_iz) > 0:\n",
    "        df_intrazonal_vmt = calculate_intrazonal_vmt(_df_iz, conn)\n",
    "        \n",
    "    else:\n",
    "        # Load the regional results and fill VMT with 0\n",
    "        df_intrazonal_vmt = pd.read_csv(os.path.join(run_dir, r'outputs\\emissions\\intrazonal_vmt_grouped.csv'))\n",
    "        df_intrazonal_vmt['VMT'] = 0\n",
    "    df_intrazonal = calculate_intrazonal_emissions(df_intrazonal_vmt, df_running_rates)\n",
    "\n",
    "    # Intersect parcels with the city gdf to get number of household to adjust vehicle starts\n",
    "    parcel_intersect_gdf = gpd.overlay(parcel_gdf,  \n",
    "                city_gdf, \n",
    "                how=\"intersection\")\n",
    "    start_emissions_df = calculate_start_emissions(conn, parcel_intersect_gdf, model_year)\n",
    "\n",
    "    df_inter_group = df_interzonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot','vmt']].reset_index()\n",
    "    df_inter_group.rename(columns={'tons_tot': 'interzonal_tons', 'vmt': 'interzonal_vmt'}, inplace=True)\n",
    "    df_intra_group = df_intrazonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot','vmt']].reset_index()\n",
    "    df_intra_group.rename(columns={'tons_tot': 'intrazonal_tons', 'vmt': 'intrazonal_vmt'}, inplace=True)\n",
    "    df_start_group = start_emissions_df.groupby(['pollutantID','veh_type']).sum()[['start_tons']].reset_index()\n",
    "\n",
    "    summary_df = pd.merge(df_inter_group, df_intra_group)\n",
    "    summary_df = pd.merge(summary_df, df_start_group, how='left')\n",
    "    summary_df = finalize_emissions(summary_df, col_suffix=\"\")\n",
    "    summary_df.loc[~summary_df['pollutantID'].isin(['PM','PM10','PM25']),'pollutantID'] = summary_df[~summary_df['pollutantID'].isin(['PM','PM10','PM25'])]['pollutantID'].astype('int')\n",
    "    summary_df['pollutant_name'] = summary_df['pollutantID'].astype('int', errors='ignore').astype('str').map(pollutant_map)\n",
    "    summary_df['total_daily_tons'] = summary_df['start_tons']+summary_df['interzonal_tons']+summary_df['intrazonal_tons']\n",
    "    summary_df = summary_df[['pollutantID','pollutant_name','veh_type','intrazonal_vmt','interzonal_vmt','start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bainbridge Island\n"
     ]
    }
   ],
   "source": [
    "# Select all cities and towns in the region\n",
    "city_list = gdf_shp[(gdf_shp['cnty_name'].isin(['King','Kitsap','Pierce','Snohomish'])) & (gdf_shp['rg_propose_pa'].isin(['CitiesTowns', 'Core',\n",
    "                'Metro','HCT']))]['juris'].to_list()\n",
    "# Exclude any PAA (potential annexation area from this list); we will consider that unincorporated areas\n",
    "for i in city_list:\n",
    "    if 'PAA' in i:\n",
    "        city_list.remove(i)\n",
    "\n",
    "# Manually remove some anomalies:\n",
    "city_list.remove('North Highline')\n",
    "\n",
    "missing_city_list = []\n",
    "\n",
    "for city in city_list:\n",
    "    if not os.path.isfile(os.path.join(output_dir,city+'.csv')):\n",
    "        print(city)\n",
    "        try:\n",
    "            _gdf_shp = gdf_shp[gdf_shp['juris'] == city]\n",
    "            df = evaluate_emissions(_gdf_shp, model_year)\n",
    "            df.to_csv(os.path.join(output_dir,city+'.csv'), index=False)\n",
    "        except:\n",
    "            print('ERROR for: ' +city)\n",
    "            missing_city_list.append(city)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2050\n",
      "2050\n",
      "2050\n",
      "2050\n"
     ]
    }
   ],
   "source": [
    "# Sometimes there are database issues when loading data;\n",
    "# If the initial pass did not work for all cities, try a second pass\n",
    "# Usually this will catch any city that did not work on the first attempt\n",
    "\n",
    "for city in missing_city_list:\n",
    "# for city in ['Bothell','Medina','Milton','Newcastle']:\n",
    "    # _gdf_shp = g?.path.join(output_dir,city+'.csv'), index=False)\n",
    "    # print(city)\n",
    "    try:\n",
    "        _gdf_shp = gdf_shp[gdf_shp['juris'] == city]\n",
    "        df = evaluate_emissions(_gdf_shp, model_year)\n",
    "        df.to_csv(os.path.join(output_dir,city+'.csv'), index=False)\n",
    "    except:\n",
    "        print('ERROR for: ' +city)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Emissions and VMT for Entirity of King County\n",
    "The remainder between the county and city totals will be unincorporated areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    }
   ],
   "source": [
    "# The shapefile used for this exercise covers the entirity of King County\n",
    "# any location in the inverse of the previously calculated cities will be considered unincorporated King County\n",
    "_gdf_shp = gdf_shp[(gdf_shp['cnty_name'] == 'King')]\n",
    "df = evaluate_emissions(_gdf_shp, model_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(output_dir,'King County Total.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(output_dir,'King County Total.csv'))\n",
    "df['vmt'] = df['interzonal_vmt'] + df['intrazonal_vmt']\n",
    "df = df.groupby(['pollutant_name','veh_type']).sum()[['total_daily_tons','vmt']].reset_index()\n",
    "df.to_csv(os.path.join(output_dir,model_year+'_county_summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize total emissions by each city\n",
    "# Note that VMT will be listed for the same city and vehicle type combination multiple times (for each pollutant)\n",
    "\n",
    "output_dir = r'C:\\Workspace\\aq_tool\\output\\\\' + model_year\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for city in city_list:\n",
    "    _df = pd.read_csv(os.path.join(output_dir,city+'.csv'))\n",
    "    _df['vmt'] = _df['interzonal_vmt'] + _df['intrazonal_vmt']\n",
    "    _df = _df.groupby(['pollutant_name','veh_type']).sum()[['total_daily_tons','vmt']].reset_index()\n",
    "    _df['city'] = city\n",
    "    df = df.append(_df)\n",
    "df.to_csv(os.path.join(output_dir,model_year+'_summary.csv'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a01578ef7fc98460838ddfd60bd3288ea700197594b0894c527f4f3349251842"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
