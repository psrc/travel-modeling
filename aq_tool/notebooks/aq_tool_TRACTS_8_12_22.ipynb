{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script computes emissions within city boundaries using standard Soundcast network outputs and established emissions rates for base and forecast years. Additional functions are provided from the imported \"emissions\" and \"functions\" methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os.path\n",
    "\n",
    "from functions import read_from_sde, load_network_summary, intersect_geog\n",
    "from emissions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root of model run to analyze AND the model year\n",
    "run_dir = r'L:\\RTP_2022\\final_runs\\sc_rtp_2018_final\\soundcast'\n",
    "model_year = '2018'    # Make sure to update this since rates used are based on this value\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_rtp_2030_final\\soundcast'\n",
    "# model_year = '2030'    # Make sure to update this since rates used are based on this value\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_2040_rtp_final\\soundcast'\n",
    "# model_year = '2040'\n",
    "# run_dir = r'\\\\modelstation1\\c$\\workspace\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "# model_year = '2050'\n",
    "\n",
    "# Set output directory; results will be stored in a folder by model year \n",
    "output_dir = r'C:\\Workspace\\aq_tool\\output_TRACT\\\\' + model_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Change working directory to run_dir\n",
    "os.chdir(run_dir)\n",
    "\n",
    "# Load the network\n",
    "crs = 'EPSG:2285'\n",
    "gdf_network = gpd.read_file(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_edges.shp'))\n",
    "gdf_network.crs = crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_conn = pyodbc.connect(conn_string)\n",
    "\n",
    "# df.to_sql(name=sql_table_name, schema=sql_schema, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  tract geographies from ElmerGeo\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "\n",
    "version = \"'DBO.Default'\"\n",
    "gdf_shp = read_from_sde(connection_string, 'tract2010', version, crs=crs, is_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parcels as a geodataframe\n",
    "parcel_df = pd.read_csv(os.path.join(run_dir,r'inputs/scenario/landuse/parcels_urbansim.txt'), delim_whitespace=True,\n",
    "                            usecols=['PARCELID','XCOORD_P','YCOORD_P'])\n",
    "\n",
    "parcel_gdf = gpd.GeoDataFrame(parcel_df,\n",
    "        geometry=gpd.points_from_xy(parcel_df['XCOORD_P'], parcel_df['YCOORD_P']), crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform intersect to get the network within each city in a list\n",
    "df_network = load_network_summary(os.path.join(run_dir, r'outputs\\network\\network_results.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_start_emissions_tract(df_veh, parcel_geog, df_hh, conn, intersect_gdf, model_year):\n",
    "    \"\"\" Calculate start emissions based on vehicle population by county and year. \"\"\"\n",
    "\n",
    "    tot_veh = df_hh['hhvehs'].sum()\n",
    "    # Scale total county vehicles owned to match model\n",
    "    tot_veh_model_base_year = 3007056\n",
    "    veh_scale = 1.0+(tot_veh - tot_veh_model_base_year)/tot_veh_model_base_year\n",
    "    df_veh['vehicles'] = df_veh['vehicles']*veh_scale\n",
    "\n",
    "    # Select total vehicles by county within the intersected geographies\n",
    "    # This will indetify the shares of vehicles per county from the spatial joined data\n",
    "    df_hh = df_hh.merge(parcel_geog, left_on='hhparcel', right_on='ParcelID')    # join hh data to parcels\n",
    "    _df_hh = df_hh[df_hh['hhparcel'].isin(intersect_gdf['PARCELID'])]    # Intersect with filtered geographic data\n",
    "    _hh_vehs = _df_hh.groupby('CountyName').sum()[['hhvehs']]    # Get total vehicles by county within filtered geog\n",
    "    \n",
    "    # Calculate percent of vehicles in each county for filtered geog versus full results by county\n",
    "    county_tot_vehs = df_hh.groupby('CountyName').sum()[['hhvehs']].reset_index()\n",
    "    subset_vehs = _df_hh.groupby('CountyName').sum()[['hhvehs']].reset_index()\n",
    "    county_subset_shares = county_tot_vehs.merge(subset_vehs, on='CountyName', how='left', suffixes=['_tot', '_subset']).fillna(0)\n",
    "    county_subset_shares['hhvehs_share'] = county_subset_shares['hhvehs_subset']/county_subset_shares['hhvehs_tot']\n",
    "    county_subset_shares['CountyName'] = county_subset_shares['CountyName'].str.lower()\n",
    "\n",
    "    # Apply shares to the total vehicles df; results are scaled # of vehicles from filtered geog within each county\n",
    "    df_veh = df_veh.merge(county_subset_shares[['CountyName','hhvehs_share']], left_on='county', right_on='CountyName', how='left')\n",
    "    df_veh['vehicles'] = df_veh['vehicles'] *df_veh['hhvehs_share']\n",
    "\n",
    "    # Join with rates to calculate total emissions\n",
    "    print(model_year)\n",
    "    start_rates_df = pd.read_sql('SELECT * FROM start_emission_rates_by_veh_type WHERE year=='+model_year, con=conn)\n",
    "\n",
    "    # Select winter rates for pollutants other than those listed in summer_list\n",
    "    df_summer = start_rates_df[start_rates_df['pollutantID'].isin(summer_list)]\n",
    "    df_summer = df_summer[df_summer['monthID'] == 7]\n",
    "    df_winter = start_rates_df[~start_rates_df['pollutantID'].isin(summer_list)]\n",
    "    df_winter = df_winter[df_winter['monthID'] == 1]\n",
    "    start_rates_df = df_winter.append(df_summer)\n",
    "\n",
    "    # Sum total emissions across all times of day, by county, for each pollutant\n",
    "    start_rates_df = start_rates_df.groupby(['pollutantID','county','veh_type']).sum()[['ratePerVehicle']].reset_index()\n",
    "\n",
    "    df = pd.merge(df_veh, start_rates_df, left_on=['type','county'],right_on=['veh_type','county'])\n",
    "    df['start_grams'] = df['vehicles']*df['ratePerVehicle'] \n",
    "    df['start_tons'] = grams_to_tons(df['start_grams'])\n",
    "    df = df.groupby(['pollutantID','veh_type','county']).sum().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select all tracts in Seattle\n",
    "\n",
    "# tract_list = gdf_intersect[gdf_intersect['city_name'] == 'Seattle']['tractce10'].unique()\n",
    "# missing_city_list = []\n",
    "\n",
    "# for tract in tract_list:\n",
    "#     if not os.path.isfile(os.path.join(output_dir,tract+'.csv')):\n",
    "#         print(tract)\n",
    "#         try:\n",
    "#             _gdf_shp = gdf_shp[gdf_shp['tractce10'] == tract]\n",
    "#             df = evaluate_emissions(_gdf_shp, model_year, tract)\n",
    "#             df.to_csv(os.path.join(output_dir,tract+'.csv'), index=False)\n",
    "#         except:\n",
    "#             print('ERROR for: ' +tract)\n",
    "#             missing_city_list.append(tract)\n",
    "#             continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_shp.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One time load and process!!\n",
    "###\n",
    "conn = create_engine('sqlite:///inputs/db/soundcast_inputs.db')\n",
    "# Load running emission rates by vehicle type, for the model year\n",
    "os.chdir(run_dir)\n",
    "df_running_rates = pd.read_sql('SELECT * FROM running_emission_rates_by_veh_type WHERE year=='+model_year, con=conn)\n",
    "df_running_rates.rename(columns={'ratePerDistance': 'grams_per_mile'}, inplace=True)\n",
    "df_running_rates['year'] = df_running_rates['year'].astype('str')\n",
    "# Select the month to use for each pollutant; some rates are used for winter or summer depending\n",
    "# on when the impacts are at a maximum due to temperature.\n",
    "df_summer = df_running_rates[df_running_rates['pollutantID'].isin(summer_list)]\n",
    "df_summer = df_summer[df_summer['monthID'] == 7]\n",
    "df_winter = df_running_rates[~df_running_rates['pollutantID'].isin(summer_list)]\n",
    "df_winter = df_winter[df_winter['monthID'] == 1]\n",
    "df_running_rates = df_winter.append(df_summer)\n",
    "\n",
    "# Get list of zones to include for intrazonal trips\n",
    "# Include intrazonal trips for any TAZ centroids within city boundary\n",
    "\n",
    "# Load TAZ centroids\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "version = \"'DBO.Default'\"\n",
    "taz_gdf = read_from_sde(connection_string, 'taz2010_no_water', version, crs=crs, is_table=False)\n",
    "\n",
    "_taz_gdf = gpd.GeoDataFrame(taz_gdf.centroid)\n",
    "_taz_gdf.geometry = _taz_gdf[0]\n",
    "_taz_gdf['taz'] = taz_gdf['taz'].astype('int')\n",
    "\n",
    "# Load intrazonal trips for zones in the area\n",
    "df_iz = pd.read_csv(os.path.join(run_dir,r'outputs\\network\\iz_vol.csv'))\n",
    "\n",
    "\n",
    "# Intersect parcels with the city gdf to get number of household to adjust vehicle starts\n",
    "parcel_intersect_gdf = gpd.overlay(parcel_gdf,  \n",
    "            gdf_shp, \n",
    "            how=\"intersection\")\n",
    "\n",
    "# Load observed base year vehicle populations by county\n",
    "df_veh = pd.read_sql('SELECT * FROM vehicle_population WHERE year=='+base_year, con=conn)\n",
    "# Load parcel to county geographic lookup\n",
    "parcel_geog = pd.read_sql(\"SELECT ParcelID, CountyName FROM parcel_\"+str(base_year)+\"_geography\", con=conn) \n",
    "\n",
    "# Scale all vehicles by difference between base year and modeled total vehicles owned from auto onwership model\n",
    "df_hh = pd.read_csv(r'outputs/daysim/_household.tsv', delim_whitespace=True, usecols=['hhvehs','hhparcel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ \n",
    "# Process one tract at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel_intersect_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_emissions(_gdf_shp, model_year, tract):\n",
    "    # Intersect jurisdiction polygon with network shapefile and network CSV file\n",
    "    _gdf_shp = intersect_geog(_gdf_shp, gdf_network, df_network)\n",
    "\n",
    "    # Select links from network summary dataframe that are within the gdf_shp\n",
    "    # The dataframe contains link-level model outputs\n",
    "    _df = df_network[df_network['ij'].isin(_gdf_shp['id'])]\n",
    "\n",
    "    # Replace length with length from _gdf_shp to ensure roads stop at city boundaries\n",
    "    # Drop \"length field from _gdf_shp, which is length in miles; \n",
    "    # use new_length field, which is calculated length of intersected links in feet\n",
    "    _df.drop('length', axis=1, inplace=True)\n",
    "    _df = _df.merge(_gdf_shp, how='left', left_on='ij', right_on='id')\n",
    "    # _df.drop('length', axis=1, inplace=True)\n",
    "    _df.rename(columns={'new_length': 'length',\n",
    "                    'length': 'original_length'}, inplace=True)\n",
    "\n",
    "    # Calculate interzonal emissions using same approach as for regional/county emissions\n",
    "    df_interzonal_vmt = calculate_interzonal_vmt(_df)\n",
    "    df_interzonal = calculate_interzonal_emissions(df_interzonal_vmt, df_running_rates)\n",
    "\n",
    "    intersect_gdf = gpd.overlay(_taz_gdf, \n",
    "            _gdf_shp, \n",
    "            how=\"intersection\")\n",
    "\n",
    "    # Filter for zone centroids within the jurisdiction\n",
    "    _df_iz = df_iz[df_iz['taz'].isin(intersect_gdf.taz)]\n",
    "\n",
    "    # If no zones centroids in a city, pass an empty df with 0 values for VMT;\n",
    "    # Otherwise calculate intrazonal VMT for the associated TAZs only\n",
    "    if len(_df_iz) > 0:\n",
    "        df_intrazonal_vmt = calculate_intrazonal_vmt(_df_iz, conn)\n",
    "\n",
    "    else:\n",
    "        # Load the regional results and fill VMT with 0\n",
    "        df_intrazonal_vmt = pd.read_csv(os.path.join(run_dir, r'outputs\\emissions\\intrazonal_vmt_grouped.csv'))\n",
    "        df_intrazonal_vmt['VMT'] = 0\n",
    "\n",
    "    df_intrazonal = calculate_intrazonal_emissions(df_intrazonal_vmt, df_running_rates)\n",
    "    _parcel_intersect_gdf = parcel_intersect_gdf[parcel_intersect_gdf['tractce10'] == tract]\n",
    "\n",
    "    start_emissions_df = calculate_start_emissions_tract(df_veh, parcel_geog, df_hh, conn, _parcel_intersect_gdf, model_year)\n",
    "\n",
    "    df_inter_group = df_interzonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot','vmt']].reset_index()\n",
    "    df_inter_group.rename(columns={'tons_tot': 'interzonal_tons', 'vmt': 'interzonal_vmt'}, inplace=True)\n",
    "    df_intra_group = df_intrazonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot','vmt']].reset_index()\n",
    "    df_intra_group.rename(columns={'tons_tot': 'intrazonal_tons', 'vmt': 'intrazonal_vmt'}, inplace=True)\n",
    "    df_start_group = start_emissions_df.groupby(['pollutantID','veh_type']).sum()[['start_tons']].reset_index()\n",
    "\n",
    "    summary_df = pd.merge(df_inter_group, df_intra_group)\n",
    "    summary_df = pd.merge(summary_df, df_start_group, how='left')\n",
    "    summary_df = finalize_emissions(summary_df, col_suffix=\"\")\n",
    "    summary_df.loc[~summary_df['pollutantID'].isin(['PM','PM10','PM25']),'pollutantID'] = summary_df[~summary_df['pollutantID'].isin(['PM','PM10','PM25'])]['pollutantID'].astype('int')\n",
    "    summary_df['pollutant_name'] = summary_df['pollutantID'].astype('int', errors='ignore').astype('str').map(pollutant_map)\n",
    "    summary_df['total_daily_tons'] = summary_df['start_tons']+summary_df['interzonal_tons']+summary_df['intrazonal_tons']\n",
    "    summary_df = summary_df[['pollutantID','pollutant_name','veh_type','intrazonal_vmt','interzonal_vmt','start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract = '000800'\n",
    "_gdf_shp = gdf_shp[gdf_shp['tractce10'] == tract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_emissions(_gdf_shp, model_year, tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect tract with city layer to get list of tracts in seattle\n",
    "gdf_tract = read_from_sde(connection_string, 'tract2010', version, crs=crs, is_table=False)\n",
    "gdf_cities = read_from_sde(connection_string, 'cities', version, crs=crs, is_table=False)\n",
    "gdf_intersect = gpd.overlay(gdf_tract, gdf_cities,how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "026700\n",
      "2018\n",
      "ERROR for: 026700\n",
      "026600\n",
      "2018\n",
      "ERROR for: 026600\n",
      "000800\n",
      "2018\n",
      "ERROR for: 000800\n",
      "000700\n",
      "ERROR for: 000700\n",
      "000600\n",
      "2018\n",
      "ERROR for: 000600\n",
      "000500\n",
      "2018\n",
      "ERROR for: 000500\n",
      "000402\n",
      "2018\n",
      "ERROR for: 000402\n",
      "000401\n",
      "2018\n",
      "ERROR for: 000401\n",
      "000300\n",
      "2018\n",
      "ERROR for: 000300\n",
      "000200\n",
      "2018\n",
      "ERROR for: 000200\n",
      "000100\n",
      "2018\n",
      "ERROR for: 000100\n",
      "026500\n",
      "2018\n",
      "ERROR for: 026500\n",
      "026400\n",
      "2018\n",
      "ERROR for: 026400\n",
      "026300\n",
      "2018\n",
      "ERROR for: 026300\n",
      "026100\n",
      "2018\n",
      "ERROR for: 026100\n",
      "026001\n",
      "2018\n",
      "ERROR for: 026001\n",
      "021300\n",
      "2018\n",
      "ERROR for: 021300\n",
      "001702\n",
      "2018\n",
      "ERROR for: 001702\n",
      "004302\n",
      "2018\n",
      "ERROR for: 004302\n",
      "004301\n",
      "2018\n",
      "ERROR for: 004301\n",
      "007402\n",
      "2018\n",
      "ERROR for: 007402\n",
      "010002\n",
      "2018\n",
      "ERROR for: 010002\n",
      "010001\n",
      "2018\n",
      "ERROR for: 010001\n",
      "011002\n",
      "2018\n",
      "ERROR for: 011002\n",
      "010402\n",
      "2018\n",
      "ERROR for: 010402\n",
      "010702\n",
      "2018\n",
      "ERROR for: 010702\n",
      "011402\n",
      "2018\n",
      "ERROR for: 011402\n",
      "021100\n",
      "2018\n",
      "ERROR for: 021100\n",
      "021000\n",
      "2018\n",
      "ERROR for: 021000\n",
      "020900\n",
      "2018\n",
      "ERROR for: 020900\n",
      "011401\n",
      "ERROR for: 011401\n",
      "012100\n",
      "ERROR for: 012100\n",
      "012000\n",
      "ERROR for: 012000\n",
      "011900\n",
      "2018\n",
      "ERROR for: 011900\n",
      "010900\n",
      "2018\n",
      "ERROR for: 010900\n",
      "010800\n",
      "2018\n",
      "ERROR for: 010800\n",
      "010600\n",
      "2018\n",
      "ERROR for: 010600\n",
      "010500\n",
      "2018\n",
      "ERROR for: 010500\n",
      "010300\n",
      "2018\n",
      "ERROR for: 010300\n",
      "001701\n",
      "2018\n",
      "ERROR for: 001701\n",
      "007401\n",
      "2018\n",
      "ERROR for: 007401\n",
      "010200\n",
      "2018\n",
      "ERROR for: 010200\n",
      "010100\n",
      "2018\n",
      "ERROR for: 010100\n",
      "009900\n",
      "2018\n",
      "ERROR for: 009900\n",
      "009800\n",
      "2018\n",
      "ERROR for: 009800\n",
      "009702\n",
      "2018\n",
      "ERROR for: 009702\n",
      "009701\n",
      "2018\n",
      "ERROR for: 009701\n",
      "009600\n",
      "2018\n",
      "ERROR for: 009600\n",
      "010401\n",
      "2018\n",
      "ERROR for: 010401\n",
      "010701\n",
      "2018\n",
      "ERROR for: 010701\n",
      "011001\n",
      "2018\n",
      "ERROR for: 011001\n",
      "009500\n",
      "2018\n",
      "ERROR for: 009500\n",
      "009400\n",
      "2018\n",
      "ERROR for: 009400\n",
      "009300\n",
      "2018\n",
      "ERROR for: 009300\n",
      "009200\n",
      "2018\n",
      "ERROR for: 009200\n",
      "009100\n",
      "2018\n",
      "ERROR for: 009100\n",
      "009000\n",
      "2018\n",
      "ERROR for: 009000\n",
      "008900\n",
      "2018\n",
      "ERROR for: 008900\n",
      "008800\n",
      "2018\n",
      "ERROR for: 008800\n",
      "008700\n",
      "2018\n",
      "ERROR for: 008700\n",
      "011800\n",
      "2018\n",
      "ERROR for: 011800\n",
      "011700\n",
      "2018\n",
      "ERROR for: 011700\n",
      "011600\n",
      "2018\n",
      "ERROR for: 011600\n",
      "011500\n",
      "2018\n",
      "ERROR for: 011500\n",
      "011300\n",
      "2018\n",
      "ERROR for: 011300\n",
      "011200\n",
      "2018\n",
      "ERROR for: 011200\n",
      "011102\n",
      "2018\n",
      "ERROR for: 011102\n",
      "011101\n",
      "2018\n",
      "ERROR for: 011101\n",
      "008600\n",
      "2018\n",
      "ERROR for: 008600\n",
      "008500\n",
      "2018\n",
      "ERROR for: 008500\n",
      "008400\n",
      "2018\n",
      "ERROR for: 008400\n",
      "008300\n",
      "2018\n",
      "ERROR for: 008300\n",
      "008200\n",
      "2018\n",
      "ERROR for: 008200\n",
      "008100\n",
      "2018\n",
      "ERROR for: 008100\n",
      "008002\n",
      "2018\n",
      "ERROR for: 008002\n",
      "008001\n",
      "2018\n",
      "ERROR for: 008001\n",
      "007900\n",
      "2018\n",
      "ERROR for: 007900\n",
      "007800\n",
      "2018\n",
      "ERROR for: 007800\n",
      "007700\n",
      "2018\n",
      "ERROR for: 007700\n",
      "007600\n",
      "2018\n",
      "ERROR for: 007600\n",
      "007500\n",
      "2018\n",
      "ERROR for: 007500\n",
      "007300\n",
      "2018\n",
      "ERROR for: 007300\n",
      "007200\n",
      "2018\n",
      "ERROR for: 007200\n",
      "007100\n",
      "2018\n",
      "ERROR for: 007100\n",
      "007000\n",
      "2018\n",
      "ERROR for: 007000\n",
      "006900\n",
      "2018\n",
      "ERROR for: 006900\n",
      "006800\n",
      "2018\n",
      "ERROR for: 006800\n",
      "006700\n",
      "2018\n",
      "ERROR for: 006700\n",
      "006600\n",
      "2018\n",
      "ERROR for: 006600\n",
      "006500\n",
      "2018\n",
      "ERROR for: 006500\n",
      "006400\n",
      "2018\n",
      "ERROR for: 006400\n",
      "006300\n",
      "2018\n",
      "ERROR for: 006300\n",
      "006200\n",
      "2018\n",
      "ERROR for: 006200\n",
      "006100\n",
      "2018\n",
      "ERROR for: 006100\n",
      "006000\n",
      "2018\n",
      "ERROR for: 006000\n",
      "005801\n",
      "2018\n",
      "ERROR for: 005801\n",
      "005700\n",
      "2018\n",
      "ERROR for: 005700\n",
      "005600\n",
      "2018\n",
      "ERROR for: 005600\n",
      "005400\n",
      "2018\n",
      "ERROR for: 005400\n",
      "005302\n",
      "2018\n",
      "ERROR for: 005302\n",
      "005301\n",
      "2018\n",
      "ERROR for: 005301\n",
      "005200\n",
      "2018\n",
      "ERROR for: 005200\n",
      "005100\n",
      "2018\n",
      "ERROR for: 005100\n",
      "005000\n",
      "2018\n",
      "ERROR for: 005000\n",
      "004900\n",
      "2018\n",
      "ERROR for: 004900\n",
      "004800\n",
      "2018\n",
      "ERROR for: 004800\n",
      "004700\n",
      "2018\n",
      "ERROR for: 004700\n",
      "004600\n",
      "2018\n",
      "ERROR for: 004600\n",
      "004500\n",
      "2018\n",
      "ERROR for: 004500\n",
      "004400\n",
      "2018\n",
      "ERROR for: 004400\n",
      "004200\n",
      "2018\n",
      "ERROR for: 004200\n",
      "004100\n",
      "2018\n",
      "ERROR for: 004100\n",
      "004000\n",
      "2018\n",
      "ERROR for: 004000\n",
      "003900\n",
      "2018\n",
      "ERROR for: 003900\n",
      "003800\n",
      "2018\n",
      "ERROR for: 003800\n",
      "005900\n",
      "2018\n",
      "ERROR for: 005900\n",
      "005802\n",
      "2018\n",
      "ERROR for: 005802\n",
      "003600\n",
      "2018\n",
      "ERROR for: 003600\n",
      "003500\n",
      "2018\n",
      "ERROR for: 003500\n",
      "003400\n",
      "2018\n",
      "ERROR for: 003400\n",
      "003300\n",
      "2018\n",
      "ERROR for: 003300\n",
      "003200\n",
      "2018\n",
      "ERROR for: 003200\n",
      "003100\n",
      "2018\n",
      "ERROR for: 003100\n",
      "003000\n",
      "2018\n",
      "ERROR for: 003000\n",
      "002900\n",
      "2018\n",
      "ERROR for: 002900\n",
      "002800\n",
      "2018\n",
      "ERROR for: 002800\n",
      "002700\n",
      "2018\n",
      "ERROR for: 002700\n",
      "002600\n",
      "2018\n",
      "ERROR for: 002600\n",
      "002500\n",
      "2018\n",
      "ERROR for: 002500\n",
      "002400\n",
      "2018\n",
      "ERROR for: 002400\n",
      "002200\n",
      "2018\n",
      "ERROR for: 002200\n",
      "002100\n",
      "2018\n",
      "ERROR for: 002100\n",
      "002000\n",
      "2018\n",
      "ERROR for: 002000\n",
      "001900\n",
      "2018\n",
      "ERROR for: 001900\n",
      "001800\n",
      "2018\n",
      "ERROR for: 001800\n",
      "001600\n",
      "2018\n",
      "ERROR for: 001600\n",
      "001500\n",
      "2018\n",
      "ERROR for: 001500\n",
      "001400\n",
      "2018\n",
      "ERROR for: 001400\n",
      "001300\n",
      "2018\n",
      "ERROR for: 001300\n",
      "001200\n",
      "2018\n",
      "ERROR for: 001200\n",
      "001100\n",
      "2018\n",
      "ERROR for: 001100\n",
      "001000\n",
      "2018\n",
      "ERROR for: 001000\n",
      "000900\n",
      "2018\n",
      "ERROR for: 000900\n"
     ]
    }
   ],
   "source": [
    "# Process multiple census tracts\n",
    "# Select all tracts in Seattle\n",
    "\n",
    "tract_list = gdf_intersect[gdf_intersect['city_name'] == 'Seattle']['tractce10'].unique()\n",
    "missing_city_list = []\n",
    "\n",
    "for tract in tract_list:\n",
    "    if not os.path.isfile(os.path.join(output_dir,tract+'.csv')):\n",
    "        print(tract)\n",
    "        try:\n",
    "            _gdf_shp = gdf_shp[gdf_shp['tractce10'] == tract]\n",
    "            df = evaluate_emissions(_gdf_shp, model_year, tract)\n",
    "            df.to_csv(os.path.join(output_dir,tract+'.csv'), index=False)\n",
    "        except:\n",
    "            print('ERROR for: ' +tract)\n",
    "            missing_city_list.append(tract)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a01578ef7fc98460838ddfd60bd3288ea700197594b0894c527f4f3349251842"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
