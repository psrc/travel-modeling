{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Daysim formatted 2014 survey with adjusted weights\n",
    "trip = pd.read_excel(r'R:\\4K\\2014\\Trip Generation\\Trip Rates\\2014-new-adjusted\\trip_2014_survey_adjusted_daysim.xlsx')\n",
    "parcel = pd.read_csv(r'Q:\\soundcast_2014\\inputs\\buffered_parcels.txt', sep=' ')\n",
    "\n",
    "#export_trip_hh = pd.merge(export_trip_hh, parcel, left_on='opcl', right_on='parcelid')\n",
    "#export_trip_hh = pd.merge(export_trip_hh, parcel, left_on='dpcl', right_on='parcelid', suffixes=['_o','_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum employment variables by TAZ\n",
    "\n",
    "# Take a weighted average by emp, pop, and emp+pop for other variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate TAZ totals for households, employment\n",
    "totals_by_taz = parcel.groupby('taz_p').sum()[[\n",
    "       'hh_p', u'stugrd_p', u'stuhgh_p', u'stuuni_p', u'empedu_p',\n",
    "       u'empfoo_p', u'empgov_p', u'empind_p', u'empmed_p', u'empofc_p',\n",
    "       u'empret_p', u'empsvc_p', u'empoth_p', u'emptot_p']]\n",
    "totals_by_taz.columns = [i.split('_p')[0]+'_taz_tot' for i in totals_by_taz.columns]\n",
    "# totals_by_taz = totals_by_taz.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calcualte the mixed use variable on parcel data\n",
    "# using daysim definition from Log2 functino\n",
    "parcel['tot_hhemp_1'] = parcel['emptot_1'] + parcel['hh_1']\n",
    "parcel['tot_hhemp_2'] = parcel['emptot_2'] + parcel['hh_2']\n",
    "\n",
    "parcel['mixed_use_1'] = -1*(((parcel['emptot_1']/parcel['tot_hhemp_1'])*np.log(parcel['emptot_1']/parcel['tot_hhemp_1']) \\\n",
    "+ (parcel['hh_1']/parcel['tot_hhemp_1'])*np.log(parcel['hh_1']/parcel['tot_hhemp_1']))/np.log(2))\n",
    "\n",
    "parcel['mixed_use_2'] = -1*(((parcel['emptot_2']/parcel['tot_hhemp_2'])*np.log(parcel['emptot_2']/parcel['tot_hhemp_2']) \\\n",
    "+ (parcel['hh_2']/parcel['tot_hhemp_2'])*np.log(parcel['hh_2']/parcel['tot_hhemp_2']))/np.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate a weighted average for each TAZ\n",
    "# based on the parcel buffers\n",
    "# This avoids bias based on TAZ boundaries\n",
    "\n",
    "# Weighted by parcel population, parcel employment, and combination of both\n",
    "# For 1st buffer (0.25 miles?)\n",
    "\n",
    "wt_fields = ['stugrd_', 'hh_',\n",
    "       u'stuhgh_', u'stuuni_', u'empedu_', u'empfoo_', u'empgov_',\n",
    "       u'empind_', u'empmed_', u'empofc_', u'empret_', u'empsvc_',\n",
    "       u'empoth_', u'emptot_', u'parkdy_', u'parkhr_', u'ppricdy',\n",
    "       u'pprichr','tstops_','mixed_use_']\n",
    "\n",
    "transit_fields = ['dist_lbus', u'dist_ebus', u'dist_crt', u'dist_fry', u'dist_lrt', u'dist_park']\n",
    "\n",
    "for field in wt_fields:\n",
    "    \n",
    "    # Get numerator of weights (values within buffered areas)\n",
    "    # to be divided by denominator of total hh/emp/hh+emp\n",
    "    \n",
    "    # Weight by parcel population, for 0.25 and 0.5 mile buffers\n",
    "    parcel[field+'hh_wt1_tot'] = parcel[field+'1']*parcel['hh_p']    # .25 mile\n",
    "    parcel[field+'hh_wt2_tot'] = parcel[field+'2']*parcel['hh_p']    # .5 mile\n",
    "    \n",
    "    # Weight by total parcel employment\n",
    "    parcel[field+'emp_wt1_tot'] = parcel[field+'1']*parcel['emptot_p']    # .25 mile\n",
    "    parcel[field+'emp_wt2_tot'] = parcel[field+'2']*parcel['emptot_p']    # .5 mile\n",
    "    \n",
    "    # Weight by households + employment\n",
    "    parcel[field+'hhemp_wt1_tot'] = parcel[field+'1']*(parcel['emptot_p']+parcel['hh_p'])# .25 mile\n",
    "    parcel[field+'hhemp_wt2_tot'] = parcel[field+'2']*(parcel['emptot_p']+parcel['hh_p'])# .5 mile\n",
    "\n",
    "for field in transit_fields:\n",
    "    \n",
    "    parcel[field+'_hh_wt_tot'] = parcel[field]*parcel['hh_p']\n",
    "    parcel[field+'_emp_wt_tot'] = parcel[field]*parcel['emptot_p']\n",
    "    parcel[field+'_hhemp_wt_tot'] = parcel[field]*(parcel['emptot_p']+parcel['hh_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stugrd_\n",
      "hh_\n",
      "stuhgh_\n",
      "stuuni_\n",
      "empedu_\n",
      "empfoo_\n",
      "empgov_\n",
      "empind_\n",
      "empmed_\n",
      "empofc_\n",
      "empret_\n",
      "empsvc_\n",
      "empoth_\n",
      "emptot_\n",
      "parkdy_\n",
      "parkhr_\n",
      "ppricdy\n",
      "pprichr\n",
      "tstops_\n",
      "mixed_use_\n"
     ]
    }
   ],
   "source": [
    "# Compute weighted average for TAZ based on \n",
    "\n",
    "parcel['emptot_plus_hh'] = parcel['emptot_p']+parcel['hh_p']\n",
    "\n",
    "for field in wt_fields:\n",
    "    print field\n",
    "    # Weights based on household size\n",
    "    totals_by_taz[field+'hh_wt1_avg'] = parcel.groupby('taz_p').sum()[field+'hh_wt1_tot']/parcel.groupby('taz_p').sum()['hh_p']\n",
    "    totals_by_taz[field+'hh_wt2_avg'] = parcel.groupby('taz_p').sum()[field+'hh_wt2_tot']/parcel.groupby('taz_p').sum()['hh_p']\n",
    "    \n",
    "    # Weights based on employment\n",
    "    totals_by_taz[field+'emp_wt1_avg'] = parcel.groupby('taz_p').sum()[field+'emp_wt1_tot']/parcel.groupby('taz_p').sum()['emptot_p']\n",
    "    totals_by_taz[field+'emp_wt2_avg'] = parcel.groupby('taz_p').sum()[field+'emp_wt2_tot']/parcel.groupby('taz_p').sum()['emptot_p']\n",
    "    \n",
    "    # weights based on housheholds + employment\n",
    "    totals_by_taz[field+'hhemp_wt1_avg'] = parcel.groupby('taz_p').sum()[field+'hhemp_wt1_tot']/parcel.groupby('taz_p').sum()['emptot_plus_hh']\n",
    "    totals_by_taz[field+'hhemp_wt2_avg'] = parcel.groupby('taz_p').sum()[field+'hhemp_wt2_tot']/parcel.groupby('taz_p').sum()['emptot_plus_hh']\n",
    "    \n",
    "for field in transit_fields:\n",
    "    totals_by_taz[field+'_hh_wt_avg'] = parcel.groupby('taz_p').sum()[field+'_hh_wt_tot']/parcel.groupby('taz_p').sum()['hh_p']\n",
    "    totals_by_taz[field+'_emp_wt_avg'] = parcel.groupby('taz_p').sum()[field+'_emp_wt_tot']/parcel.groupby('taz_p').sum()['emptot_p']\n",
    "    totals_by_taz[field+'_hhemp_wt_avg'] = parcel.groupby('taz_p').sum()[field+'_hhemp_wt_tot']/parcel.groupby('taz_p').sum()['emptot_plus_hh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NaNs with zeros - for zones with 0 hh or emp, divide by 0 results in NaN\n",
    "totals_by_taz = totals_by_taz.fillna(0)\n",
    "\n",
    "# Join totals by taz to the trip file\n",
    "trip = pd.read_excel(r'R:\\4K\\2014\\Trip Generation\\Trip Rates\\2014-new-adjusted\\trip_2014_survey_adjusted_daysim.xlsx')\n",
    "# trip = pd.read_csv(r'J:\\Projects\\Surveys\\HHTravel\\Survey2014\\Data\\Final database\\Release 4\\Adjusted\\trips_2014_adjusted_wt_daysim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write TAZ totals to file\n",
    "totals_by_taz.to_csv(r'R:\\4K\\2014\\Trip Generation\\Trip Rates\\2014-new-adjusted\\4k_2014_taz_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exclude trips to or from external zones\n",
    "trip = trip[trip['otaz'] != 0]\n",
    "trip = trip[trip['dtaz'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_taz['taz_p'] = totals_by_taz.index\n",
    "df = pd.merge(trip, totals_by_taz, left_on='otaz', right_on='taz_p', how='left')\n",
    "df = pd.merge(df, totals_by_taz, left_on='dtaz', right_on='taz_p', how='left', suffixes=['_o','_d'])\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join transit access mode to data to split transit mode into walk and auto-access\n",
    "tour = pd.read_csv(r'R:\\SoundCastDocuments\\2014Estimation\\Files_From_Mark_2014\\new_weights_10_28_16\\skims_attached\\tourP14.dat',\n",
    "                   delim_whitespace=True)\n",
    "\n",
    "df = pd.merge(df, tour[['hhno','pno','day','tour','tmodetp']], on=['hhno','pno','day','tour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'R:\\4K\\2014\\Trip Generation\\Trip Rates\\2014-new-adjusted\\trip_2014_adjusted_lu_vars_daysim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
