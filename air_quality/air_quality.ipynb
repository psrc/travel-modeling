{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'L:\\T2040\\soundcast_2014\\outputs\\daysim\\_trip.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['person_id'] = df['hhno'].astype('str') + '_' + df['pno'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.DataFrame()\n",
    "activity.index = [[],[]]\n",
    "\n",
    "for person_id in df['person_id'].unique()[0:500]:\n",
    "    trip_subsample = df[df['person_id'] == person_id]\n",
    "    for row in xrange(0, len(trip_subsample)-1):\n",
    "        row_index = 0\n",
    "        current_row = trip_subsample.iloc[row]\n",
    "        next_row = trip_subsample.iloc[row+1]\n",
    "\n",
    "        # Activity from midnight until first trip\n",
    "        if row == 0:\n",
    "            activity.ix[(person_id,0),'person_id'] = person_id\n",
    "            activity.ix[(person_id,0),'activity'] = current_row['opurp']\n",
    "            activity.ix[(person_id,0),'activity_index'] = 0\n",
    "            activity.ix[(person_id,0),'parcel'] = current_row['opcl']\n",
    "            activity.ix[(person_id,0),'begin_time'] = 0 # minutes after midnight\n",
    "            activity.ix[(person_id,0),'end_time'] = current_row['deptm']\n",
    "            activity.ix[(person_id,0),'duration'] = activity.ix[(person_id,0),'end_time']-activity.ix[(person_id,0),'begin_time']\n",
    "            row_index += 1\n",
    "       \n",
    "        # Second activity\n",
    "        activity.ix[(person_id,row+row_index),'person_id'] = person_id\n",
    "        activity.ix[(person_id,row+row_index),'activity'] = current_row['dpurp']\n",
    "        activity.ix[(person_id,row+row_index),'activity_index'] = row+row_index\n",
    "        activity.ix[(person_id,row+row_index),'parcel'] = next_row['opcl']\n",
    "        activity.ix[(person_id,row+row_index),'begin_time'] = current_row['arrtm']\n",
    "        activity.ix[(person_id,row+row_index),'end_time'] = next_row['deptm'] # minutes after midnight\n",
    "        activity.ix[(person_id,row+row_index),'duration'] = activity.ix[(person_id,row+row_index),'end_time']-activity.ix[(person_id,row+row_index),'begin_time']\n",
    "            \n",
    "        # for last trip of the day\n",
    "        if row == len(trip_subsample)-2:\n",
    "            activity.ix[(person_id,row+2),'activity_index'] = row+2\n",
    "            activity.ix[(person_id,row+2),'activity'] = next_row['dpurp']\n",
    "            activity.ix[(person_id,row+2),'person_id'] = person_id\n",
    "            activity.ix[(person_id,row+2),'parcel'] = next_row['dpcl']\n",
    "            activity.ix[(person_id,row+2),'begin_time'] = next_row['arrtm'] # minutes after midnight\n",
    "            activity.ix[(person_id,row+2),'end_time'] = 1440 # minutes after midnight\n",
    "            activity.ix[(person_id,row+2),'duration'] = 1440-activity.ix[(person_id,row+2),'begin_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further separate each activity to 12 time periods\n",
    "# Probably need 15 minutes increments\n",
    "activity['begin_hour'] = np.round(activity['begin_time']/60.0).astype('int')\n",
    "activity['end_hour'] = np.round(activity['end_time']/60.0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity.to_csv('c:/users/brice/person_activity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split activities into equal 24 hour portions\n",
    "hourly_activity = pd.DataFrame()\n",
    "hourly_activity.index = [[],[]]\n",
    "person_list = pd.unique(activity['person_id']) # temporarily reduce list zize\n",
    "for person_id in person_list:\n",
    "    activity_list = activity[activity['person_id'] == person_id]\n",
    "    for row in xrange(len(activity_list)):\n",
    "        activity_row = activity_list.iloc[row]\n",
    "#         print '---------'\n",
    "        for hour in xrange(activity_row.begin_hour,activity_row.end_hour+1):\n",
    "#             print hour\n",
    "            hourly_activity.ix[(activity_row['person_id'],hour),'hour'] = hour\n",
    "            hourly_activity.ix[(activity_row['person_id'],hour),'parcel'] = activity_row.parcel\n",
    "            hourly_activity.ix[(activity_row['person_id'],hour),'person_id'] = activity_row.person_id\n",
    "            \n",
    "            # Add a percent of hour at this location field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look up hourly exposure rates from parcel-based table; multiply by time spent at location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bufferd parcels (500ft) intersected with (AM) network componenets for 2014\n",
    "df = pd.read_csv(r'R:/aq/parcel_network_2014_new.txt')\n",
    "# Work with a subsample for now\n",
    "# df = df.iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lookup table for hourly emissions per link\n",
    "#### for now, make this up\n",
    "# import pysal as ps\n",
    "# import numpy as np\n",
    "# links = ps.open(r'R:\\SoundCast\\Inputs\\2014\\networks\\edges_0.dbf')\n",
    "# d = dict([(col, np.array(links.by_col(col))) for col in links.header])\n",
    "# links = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with hourly network results\n",
    "network_results = pd.read_csv(r'L:\\T2040\\soundcast_2014\\outputs\\network\\network_results.csv')\n",
    "# Multiple results from past runs sometimes included, drop those\n",
    "network_results = network_results.drop_duplicates(subset=['ij','tod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of volume near each affected parcel for each time period\n",
    "parcel_vol = pd.merge(df,network_results,left_on=['NewINode','NewJNode'],right_on=['i','j'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape volumes to be for all 24 hours\n",
    "# dictionary to relate time period to 24 hours\n",
    "tod_dict = {0: '20to5',\n",
    "    1: '20to5',\n",
    "    2: '20to5',\n",
    "    3: '20to5',\n",
    "    4: '20to5',\n",
    "    5: '5to6',\n",
    "    6: '6to7',\n",
    "    7: '7to8',\n",
    "    8: '8to9',\n",
    "    9: '9to10',\n",
    "    10: '10to14',\n",
    "    11: '10to14',\n",
    "    12: '10to14',\n",
    "    13: '10to14',\n",
    "    14: '14to15',\n",
    "    15: '15to16',\n",
    "    16: '16to17',\n",
    "    17: '17to18',\n",
    "    18: '18to20',\n",
    "    19: '18to20',\n",
    "    20: '20to5',\n",
    "    21: '20to5',\n",
    "    22: '20to5',\n",
    "    23: '20to5'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_activity['tod'] = hourly_activity['hour'].astype('int')\n",
    "hourly_activity.replace({\"tod\":tod_dict},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(activity['parcel'].iloc[0])\n",
    "hourly_activity['parcel'] = hourly_activity['parcel'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop parcerls that don't show up in hourly_activity file to reduce file size\n",
    "parcel_vol_trimmed = parcel_vol[parcel_vol['parcelid'].isin(pd.unique(hourly_activity.parcel))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df = pd.merge(hourly_activity,parcel_vol_trimmed,left_on=['parcel','tod'],right_on=['parcelid','tod'], how='left')\n",
    "_df = pd.merge(hourly_activity,parcel_vol_trimmed,left_on=['parcel','tod'],right_on=['parcelid','tod'])\n",
    "_df['hour'] = _df.hour.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the hourly rates to compute total emissions accumulated over time\n",
    "\n",
    "# Load some test rates\n",
    "rates = pd.read_csv(r':\\T2040\\soundcast_2014\\outputs\\aq_2014_july.csv')\n",
    "\n",
    "# Need to use real MOVES rates here: \n",
    "# e.g., L:\\T2040\\soundcast_2014\\scripts\\summarize\\inputs\\network_summary\\emission_rates_2014.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rates.hourId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.merge(_df,rates,left_on=['hour','ij'], right_on=['hourId','ij'], how='left')\n",
    "newdf = pd.merge(_df,rates,left_on=['hour','ij'], right_on=['hourId','ij'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure what the columns mean for now; just test\n",
    "newdf['pm_10'] = newdf['total_volume']*newdf['100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_id\n",
       "101_1      85.547061\n",
       "101_6       7.389015\n",
       "104_2      91.797819\n",
       "105_1     163.061594\n",
       "107_2      65.964929\n",
       "108_1      16.232875\n",
       "108_2       8.986246\n",
       "114_2      83.287750\n",
       "116_2     198.049382\n",
       "118_2     403.284163\n",
       "121_2     226.369240\n",
       "123_1     199.593877\n",
       "124_2     215.516335\n",
       "126_2     486.689488\n",
       "132_2     706.849239\n",
       "133_1       1.798610\n",
       "134_1     165.017121\n",
       "135_4     116.121462\n",
       "136_1     241.844457\n",
       "141_1    1163.870077\n",
       "142_1     992.181418\n",
       "143_1     511.212682\n",
       "145_1      32.967992\n",
       "147_1     122.984420\n",
       "150_1     527.829551\n",
       "154_1     136.529126\n",
       "155_1      46.039257\n",
       "157_1      24.010563\n",
       "161_1    1225.136012\n",
       "163_1     498.198629\n",
       "            ...     \n",
       "44_1       42.159348\n",
       "45_1     2694.863909\n",
       "46_1       34.557095\n",
       "47_1       65.108869\n",
       "48_1       16.726594\n",
       "4_1       199.426815\n",
       "50_1      207.132538\n",
       "50_2      257.072613\n",
       "54_1     2624.457955\n",
       "56_1      542.349707\n",
       "58_1       32.972852\n",
       "5_1        74.984174\n",
       "61_1      261.368021\n",
       "65_4      313.045092\n",
       "66_1      124.764212\n",
       "66_5      198.306863\n",
       "66_6      184.860915\n",
       "67_1     2477.946906\n",
       "69_2      877.500642\n",
       "69_4      103.999593\n",
       "70_1      239.755239\n",
       "71_1      111.907250\n",
       "75_1      361.770265\n",
       "79_1      176.066754\n",
       "81_1      294.079560\n",
       "84_1      233.103009\n",
       "88_1     3635.588688\n",
       "92_1      806.239559\n",
       "94_1      197.726375\n",
       "99_1      195.376593\n",
       "Name: pm_10, dtype: float64"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.groupby('person_id').sum()['pm_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By attaching this to the person record we can analyze results by household, workplace location,\n",
    "# worker type, income, (equity geographies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
