{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transportation Emissions Exposure Analysis\n",
    "Sum of daily exposure to emissions from transportation sources using ABM outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trip records to create activity patterns for each simulated person in the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('_trip.tsv', sep='\\t')\n",
    "\n",
    "# Generate unique person ID field \n",
    "df['person_id'] = df['hhno'].astype('str') + '_' + df['pno'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the total number of trips per person, from the person records\n",
    "tot_trips = df[['person_id','trexpfac']].groupby('person_id').sum().reset_index()\n",
    "tot_trips.columns = ['person_id','total_trips']\n",
    "\n",
    "df = pd.merge(df,tot_trips,on='person_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# first trip departure is the end of the first activity; first origin parcel is location of first activity\n",
    "first_trip = df.groupby(['person_id']).first()[['opcl','deptm']].reset_index()\n",
    "first_trip.columns = ['person_id','parcel','end_time']\n",
    "\n",
    "# Save this as the first activity for each person\n",
    "activity = first_trip.copy()\n",
    "activity['begin_time'] = 0\n",
    "activity['activity_index'] = 0\n",
    "\n",
    "# Group trips by person_id and iterate through each row of grouped results to get activities\n",
    "max_trips_per_person = df['total_trips'].max()    # There are some people with 32 trips per day, may want to limit this...\n",
    "\n",
    "for i in xrange(2,max_trips_per_person+1):    # Start with the second trip since we alreayd calculated the first\n",
    "#     print i\n",
    "    current_trip = df.groupby(['person_id']).nth(n=i-1)[['opcl','dpcl','arrtm','deptm','total_trips']].reset_index()\n",
    "    activity_row = current_trip[['person_id','opcl','deptm']]    \n",
    "    activity_row.columns = ['person_id','parcel','end_time']    # activity ends when trip from current locations starts\n",
    "    \n",
    "    # Use previous trip record to define activity begingging\n",
    "    previous_trip = df.groupby(['person_id']).nth(n=i-2)[['arrtm']].reset_index()    # nth function is 0-based\n",
    "    previous_trip.columns = ['person_id','begin_time']    # activity starts when previous trip arrives at past location\n",
    "    \n",
    "    # Merge info from current and previous trips to produce a complete activity record\n",
    "    merged = pd.merge(activity_row, previous_trip, on='person_id', how='left')\n",
    "    merged['activity_index']=i-1    # use 0-based index\n",
    "    \n",
    "    # add this activity to the dataframe\n",
    "    activity = activity.append(merged)\n",
    "    \n",
    "    # For records where the current trip is the final trip, add the last activity\n",
    "    last_activity_row = current_trip[current_trip['total_trips'] == i]    # use num of total trips to identifiy last trip rows\n",
    "    if len(last_activity_row) > 0:\n",
    "        last_activity_row = last_activity_row[['person_id','dpcl','arrtm']]    \n",
    "        last_activity_row.columns = ['person_id','parcel','begin_time'] # use the arrival time and dpcl to get final activity location and start time\n",
    "        last_activity_row['end_time'] = 24*60    # End of last activity is 24 hours\n",
    "        last_activity_row['activity_index'] = i    # Add the 0-bsaed index\n",
    "\n",
    "        # add this last activity to the dataframe\n",
    "        activity = activity.append(last_activity_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further separate each activity to 12 time periods to get hourly air quality estimates\n",
    "# use floor to define the hour bin\n",
    "activity['begin_hour'] = np.floor(activity['begin_time']/60.0).astype('int')\n",
    "activity['end_hour'] = np.floor(activity['end_time']/60.0).astype('int')\n",
    "\n",
    "# Keep track of what percent of activity occurred during this hour\n",
    "activity['begin_hour_fraction'] = (activity['begin_time']-(activity['begin_hour']*60))/60\n",
    "activity['end_hour_fraction'] = (activity['end_time']-((activity['end_hour'])*60))/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "activity.to_csv(r'C:/users/brice/activity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to re-read\n",
    "activity = pd.read_csv(r'C:/users/brice/activity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Network Results to Activities\n",
    "At the Census block level, calculate total hourly emissions exposure for each block by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity results are at parcel level - aggregate to block level for now\n",
    "block_parcel_lookup = pd.read_csv(r'R:\\aq\\new_parcel_block_lookup.txt')\n",
    "\n",
    "# Add census block field (GEOID10) to the activity records\n",
    "# We use this to help filter out blocks that don't appear in the activity df\n",
    "activity = pd.merge(activity, block_parcel_lookup[['parcelid','GEOID10']], left_on='parcel',right_on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intersection of blocks and network components (replace with parcel intersect in future)\n",
    "# This was done in GIS, as an intersect between edges_0 and a layer of block centroids buffered at 500 ft.\n",
    "# Ideally do this in code with geopandas\n",
    "block_network = pd.read_csv(r'R:\\aq\\block_network_intersect.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unneeded columns and rename temporarily\n",
    "df = block_network[['Shape_Length','GEOID10','NewINode','NewJNode']]\n",
    "\n",
    "# Remove any block that doesn't exist in activity dataframe\n",
    "df = df[df['GEOID10'].isin(pd.unique(activity['GEOID10']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load emissions rates and join with network volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air quality output produced from Soundcast scripts\\summarize\\standard\\air_quality script\n",
    "aq_rates = pd.read_csv(r'L:\\T2040\\soundcast_2014\\outputs\\aq_2014_july.csv')\n",
    "\n",
    "# List of pollutant IDs; not sure which ones we will need in the future\n",
    "pollutant_list = [1,2,3,5,6,79,87,90,91,98,100,106,107,110,112,115,116,117,118,119]\n",
    "\n",
    "# Reduce column size to only include pollutant totals, nodes, volume, and hour\n",
    "aq_rates = aq_rates[['inode_x','jnode_x','total_volume','hourId']+[str(i) for i in pollutant_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the network/block intersection with the hourly rates\n",
    "block_rates = pd.merge(df, aq_rates, left_on=['NewINode','NewJNode'], right_on=['inode_x','jnode_x'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total grams emitted within the time frame\n",
    "# by multiplying total volume by miles of network link\n",
    "for pollutant in pollutant_list:\n",
    "    block_rates[str(pollutant)+'_total_grams'] = block_rates['total_volume']*(block_rates['Shape_Length']/5280)*block_rates[str(pollutant)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take sum of hourly emissions within each census block\n",
    "total_hourly_block_grams = block_rates.groupby(['GEOID10','hourId']).sum()[['100_total_grams','1_total_grams']]\n",
    "total_hourly_block_grams = total_hourly_block_grams.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollution totals by assignment period\n",
    "Rates are given at soundcast assignment periods\n",
    "Take the average for each hour to calcualte hourly exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_emissions_to_hours(df, hour_list):\n",
    "    \n",
    "    # First hour contains information summed for all time periods\n",
    "    copy_df = df[df['hourId'] == hour_list[0]].copy()\n",
    "    \n",
    "    for hour in hour_list:\n",
    "        _df = copy_df.copy()\n",
    "#         print hour\n",
    "        if hour == hour_list[0]:\n",
    "            df = df[df['hourId'] != hour_list[0]]\n",
    "        _df['hourId'] = hour\n",
    "        _df[['100_total_grams','1_total_grams']]/(len(hour_list))\n",
    "        df = df.append(_df)\n",
    "        \n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_emissions_total = average_emissions_to_hours(total_hourly_block_grams, hour_list=[10,11,12,13])\n",
    "hourly_emissions_total = average_emissions_to_hours(hourly_emissions_total, hour_list=[18,19])\n",
    "hourly_emissions_total = average_emissions_to_hours(hourly_emissions_total, hour_list=[20,21,22,23,0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file ?\n",
    "hourly_emissions_total.to_csv(r'R:/aq/hourly_emissions_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate hourly totals by activity\n",
    "hourly_emissions_total is the sum of emissions (grams) released during each hour by block\n",
    "We need to join this to the activity list to get the total per person per day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a single person's activity\n",
    "# activity_sample  = activity[activity['person_id'] == '9_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total grams for a given block and time period\n",
    "\n",
    "def total_activity_emissions(df, zone_num, emissions_type, begin_hour, begin_hour_share, end_hour, end_hour_share,\n",
    "                            geography_field='GEOID10'):\n",
    "    \"\"\"Calculate the total grams per each activity\"\"\"\n",
    "    \n",
    "    df = hourly_emissions_total\n",
    "    \n",
    "    # Totals from first hour\n",
    "    first_hour_total = df[(df[geography_field] == zone_num) & (df.hourId == begin_hour)][emissions_type].values[0]\n",
    "    first_hour_total = first_hour_total*begin_hour_share    # Modify with % of hour at that location\n",
    "    \n",
    "    # Totals from last hour\n",
    "    last_hour_total = df[(df[geography_field] == zone_num) & (df.hourId == end_hour)][emissions_type].values[0]\n",
    "    last_hour_total = last_hour_total*end_hour_share    # Modify with % of hour at that location\n",
    "    \n",
    "    # Calculate totals for interim hours if necessary\n",
    "    interim_total = 0\n",
    "    if end_hour-begin_hour>1:\n",
    "        for hour in xrange(begin_hour+1,end_hour):\n",
    "            interim_total +=  df[(df[geography_field] == zone_num) & (df.hourId == hour)][emissions_type].values[0]\n",
    "            \n",
    "    activity_total = first_hour_total + interim_total + last_hour_total\n",
    "    \n",
    "    return activity_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt in 'zmq.backend.cython.message.Frame.__dealloc__' ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-6055a4cfd143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Only include activities that occur within areas that have pollution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactivity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GEOID10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhourly_emissions_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GEOID10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36misin\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   2489\u001b[0m         \"\"\"\n\u001b[0;32m   2490\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbetween\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclusive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[1;32m--> 243\u001b[1;33m                                        raise_cast_failure=True)\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m_sanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m   2853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;31m# GH #846\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I can't figure out how to use lambda functino for a full dataframe right now,\n",
    "# so let's loop for now ...\n",
    "\n",
    "# Only include activities that occur within areas that have pollution\n",
    "df = activity[activity['GEOID10'].isin(pd.unique(hourly_emissions_total['GEOID10']))]\n",
    "\n",
    "results = []\n",
    "for i in xrange(len(df)):\n",
    "    print i\n",
    "    row = df.iloc[i]\n",
    "    tot_emissions = total_activity_emissions(df, zone_num=row['GEOID10'], emissions_type='1_total_grams', \n",
    "                             begin_hour=row['begin_hour'], begin_hour_share=row['begin_hour_fraction'], \n",
    "                             end_hour=row['end_hour'], end_hour_share=row['end_hour_fraction'])\n",
    "    results.append(tot_emissions)\n",
    "    \n",
    "df['total_exposure'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('person_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOOHOO - total daily emissions exposure by person!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Analysis:\n",
    "- attach activity type to each location\n",
    "- other things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
